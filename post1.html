<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Understanding Bayesian Inference & Applications in Social Media Analytics</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <style>
    body {
      font-family: 'Helvetica Neue', Arial, sans-serif;
      background-color: #f7f7f7;
      color: #333;
      margin: 0;
      padding: 0;
    }
    header {
      background-color: #ffffff;
      border-bottom: 1px solid #e0e0e0;
      padding: 20px 40px;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header .site-title {
      font-size: 26px;
      font-weight: 300;
      color: #555;
    }
    header .site-title a {
      text-decoration: none;
      color: inherit;
    }
    nav a {
      margin-left: 20px;
      text-decoration: none;
      font-size: 16px;
      color: #007acc;
    }
    nav a:hover {
      color: #005f99;
    }
    .container {
      max-width: 800px;
      margin: 40px auto;
      background-color: #ffffff;
      padding: 30px;
      border-radius: 8px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    }
    h2 {
      font-size: 22px;
      margin-bottom: 20px;
      color: #444;
    }
    p {
      color: #666;
      line-height: 1.6;
    }
    footer {
      background-color: #f8f8f8;
      color: #333;
      padding: 20px 0;
      text-align: center;
      font-size: 14px;
    }
    footer .social-icons {
      margin-top: 10px;
    }
    footer .social-icons a {
      color: #333;
      font-size: 24px;
      margin: 0 15px;
      text-decoration: none;
    }
    footer .social-icons a:hover {
      color: #007acc;
    }
    pre.python {
    background-color: #f5f5f5;  /* Light gray background */
    border: 1px solid #ccc;     /* Light gray border */
    padding: 15px;               /* Padding around the code */
    font-family: 'Courier New', Courier, monospace; /* Monospaced font */
    font-size: 14px;             /* Slightly larger font for readability */
    overflow-x: auto;            /* Horizontal scroll for long lines */
    white-space: pre-wrap;       /* Wrap long lines to prevent overflow */
    border-radius: 5px;          /* Rounded corners */
  }
  
  /* Optional: Styling for code inside the <pre> block */
  pre.python code {
    display: block;
    color: #333;  /* Dark text color */
  }
  </style>
</head>
<body>
  <header>
    <div class="site-title"><a href="index.html">Beyond the Basics: Statistics & Applications</a></div>
    <nav>
      <a href="posts.html">Posts</a>
      <a href="about.html">About</a>
    </nav>
  </header>
  <div class="container">
    <h1>Understanding Bayesian Inference & Applications in Social Media Analytics</h1>
    <p>Bayesian inference is a statistical framework that updates beliefs about uncertain quantities as new data become available. It is based on <strong>Bayes’ theorem</strong>, formulated by the Reverend <strong>Thomas Bayes</strong> in the 18th century and later formalized by <strong>Pierre-Simon Laplace</strong>. Unlike frequentist methods, which rely on fixed parameters and long-run frequencies, Bayesian inference treats parameters as probability distributions, allowing for a natural way to incorporate prior knowledge and quantify uncertainty.</p>
    <p>Despite its early development, Bayesian methods remained computationally challenging until the advent of <strong>Markov Chain Monte Carlo (MCMC)</strong> techniques in the 20th century, which made complex Bayesian models practical. Today, Bayesian inference is widely applied across fields such as machine learning, economics, medicine, and social sciences, offering a powerful framework for decision-making under uncertainty.</p>
    
    <h2>Introduction to Bayesian Models</h2>
    <p>Bayesian models form a cornerstone of modern statistical inference by allowing us to combine prior knowledge with observed data to make probabilistic statements about unknown parameters. In this framework, every unknown quantity is treated as a random variable. We start with a <strong>prior distribution</strong> \(p(\theta)\) that encodes our beliefs about the parameter \(\theta\) before seeing any data. Next, we incorporate the <strong>likelihood</strong> \(p(D \mid \theta)\), which describes how likely the observed data \(D\) is given \(\theta\). By applying <strong>Bayes’ theorem</strong>, we update our beliefs, resulting in the <strong>posterior distribution</strong>:</p>
    <p>\[
    p(\theta \mid D) = \frac{p(D \mid \theta) \, p(\theta)}{p(D)} \quad \text{where} \quad p(D) = \int p(D \mid \theta) \, p(\theta) \, d\theta.
    \]</p>
    <p>This posterior distribution provides a complete summary of our updated beliefs, capturing not just point estimates but also the uncertainty about \(\theta\). The beauty of the Bayesian approach is that it naturally supports sequential learning—allowing us to update our beliefs as new data arrives—and offers a principled way to quantify uncertainty in predictions.</p>
    
    <h2>Mathematical Foundations of Bayesian Inference</h2>
    <h3>Prior, Likelihood, and Posterior</h3>
    <ul>
        <li><strong>Prior \(p(\theta)\):</strong> Represents our beliefs about the unknown parameter \(\theta\) before observing any data.</li>
        <li><strong>Likelihood \(p(D \mid \theta)\):</strong> Describes the probability of the observed data \(D\) given \(\theta\).</li>
        <li><strong>Posterior \(p(\theta \mid D)\):</strong> The updated belief about \(\theta\) after observing data, obtained by applying Bayes’ theorem:</li>
    </ul>
    <p>\[
    p(\theta \mid D) = \frac{p(D \mid \theta) \, p(\theta)}{p(D)}.
    \]</p>
    
    <h2>Applications in Social Media Analytics</h2>
    <p>Social media platforms generate vast amounts of unstructured data every day—from tweets and Facebook posts to forum discussions and product reviews. Analyzing such data provides insights into public opinion, emerging events, and patterns of social influence. Bayesian methods offer a natural framework to tackle these challenges by combining prior knowledge with incoming data and quantifying uncertainty in predictions.</p>
    
    <h3>Bayesian Logistic Regression for Sentiment Analysis</h3>
    <p>For sentiment analysis, we aim to classify text (e.g., tweets) as expressing positive (\(y=1\)) or negative (\(y=0\)) sentiment. The logistic regression model is:</p>
    <p>\[
    \text{logit}(P(y=1 \mid x, \theta)) = \alpha + \beta \, x,
    \]</p>
    <p>where \(x\) is a predictor (such as a sentiment score), \(\alpha\) is the intercept, and \(\beta\) is the slope.</p>
    
    <h3>Python Example Using PyMC3</h3>
    <pre class="python"><code>
import numpy as np
import pymc3 as pm
import matplotlib.pyplot as plt
import arviz as az

# 1. Set seed for reproducibility
np.random.seed(42)

# 2. Simulate synthetic data
N = 100  # number of social media posts
x = np.random.normal(0, 1, size=N)  # a feature (e.g., sentiment score)

# True model parameters (for simulation)
true_intercept = -1.0
true_slope = 2.5
logit_p = true_intercept + true_slope * x
p_true = 1 / (1 + np.exp(-logit_p))
y = np.random.binomial(1, p_true, size=N)  # binary sentiment outcome

# 3. Build Bayesian logistic regression model using PyMC3
with pm.Model() as model:
    # Priors for the intercept and slope
    intercept = pm.Normal(&#39;intercept&#39;, mu=0, sigma=10)
    slope = pm.Normal(&#39;slope&#39;, mu=0, sigma=10)
    
    # Logistic model likelihood
    p = pm.math.sigmoid(intercept + slope * x)
    likelihood = pm.Bernoulli(&#39;y&#39;, p=p, observed=y)
    
    # 4. Sample from the posterior
    trace = pm.sample(2000, tune=1000, target_accept=0.95, return_inferencedata=True)

# 5. Plot the posterior distributions of the parameters
az.plot_trace(trace)
plt.tight_layout()
plt.show()

# 6. Summary statistics of the posterior
print(az.summary(trace, round_to=2))</code></pre>
    
    <h2>Conclusion</h2>
    <p>Bayesian models provide a flexible and robust framework for analyzing social media data and beyond. This article introduced the fundamental concepts behind Bayesian inference, derived the posterior distribution using Bayes’ theorem, and illustrated a practical example with Bayesian logistic regression for sentiment analysis.</p>
    <p>By combining prior knowledge with new data, Bayesian inference not only delivers point estimates but also a full characterization of uncertainty, making it an essential tool for modern data analytics in rapidly changing environments.</p>
<div style="display: flex; justify-content: space-between; margin-top: 20px;">
      <a href="index.html">&larr; Back to Home</a>
      <a href="posts.html">Back to Posts &rarr;</a>
    </div>
  </div>
  <footer>
    <p>&copy; 2025 Kalani Hasanthika. All rights reserved.</p>
    <div class="social-icons">
      <a href="https://github.com/khasanthika" target="_blank" class="fab fa-github"></a>
      <a href="https://www.linkedin.com/in/kalani-hasanthika-366b39b3/" target="_blank" class="fab fa-linkedin"></a>
      <a href="https://twitter.com/yourusername" target="_blank" class="fab fa-twitter"></a>
    </div>
  </footer>
</body>
</html>
